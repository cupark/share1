{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import librosa.display\n",
    "import struct\n",
    "import numpy as np \n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "                     \n",
    "        #print(\"num_channels : {}, sample_rate : {}, bit_depth : {}\".format(num_channels, sample_rate, bit_depth))\n",
    "        return (num_channels, sample_rate, bit_depth)\n",
    "#from helpers.wavfilehelper import WavFileHelper\n",
    "\n",
    "def extract_features(file_name, max_pad=174, n=40):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n)\n",
    "        # padding\n",
    "        pad_width = max_pad - mfccs.shape[1]\n",
    "        mfccs_pad = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs, mfccs_pad, mfccsscaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_SIZE = (15, 10)\n",
    "\n",
    "def draw_chart(signal, sample_rate):\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    plt.xlabel(\"Time(s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.title(\"Wave form\")\n",
    "    librosa.display.waveshow(signal, sample_rate, alpha = 0.4)\n",
    "    \n",
    "def draw_chart_half(signal, signal_spectrum):\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.title(\"Power Spectrum\")\n",
    "    plt.plot(signal, signal_spectrum, alpha=0.4)\n",
    "    \n",
    "    \n",
    "def draw_chart_spectrogram(spectrogram, sample_rate, hop_length):\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    librosa.display.specshow(spectrogram, sr=sample_rate, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Spectrogram\")\n",
    "    \n",
    "    \n",
    "def draw_chart_log_spectrogram(log_spectrogram, sample_rate, hop_length):\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    librosa.display.specshow(log_spectrogram, sr=sample_rate, hop_length =hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.colorbar(format=\"%+2.0f dB\")\n",
    "    plt.title(\"spectrogram * dB\")\n",
    "    \n",
    "def draw_chart_mfccs(mfccs, signal, sample_rate, hop_length):\n",
    "    plt.figure(figsize=FIG_SIZE)\n",
    "    librosa.display.specshow(mfccs, sr=sample_rate, hop_length=hop_length)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"MFCC Coefficients\")\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MFCCS\")\n",
    "    \n",
    "def draw_chart_input_data(sound_sample):    \n",
    "    plt.title(\"input sound\")\n",
    "    librosa.display.specshow(sound_sample[0]['mfccs'], x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Feature Extract\n",
    "'''\n",
    "        mfccs parameters \n",
    "        1. sample rate\n",
    "            sample rate = Number of Sample / time\n",
    "            ex) 3초에 90000개의 샘플 -> sr = 30kHz\n",
    "            \n",
    "        2. n_mfcc (선택)\n",
    "            n_mfcc는 리턴할 mfcc의 개수, Default = 20. \n",
    "            n_mfcc가 어느정도 많을 수록 Feature가 잘 나타난다.\n",
    "            \n",
    "        3. n_fft (Sample Rate 필요)\n",
    "            n_fft는 Frame Length를 결정한다. 또한 n_fft는 window size로 자동 결정된다. \n",
    "            n_fft > 인 경우 window size는 0을 Padding하여 계산한다. \n",
    "            따라서 n_fft >= window size를 지켜야 한다. \n",
    "            가청 주파수를 기준으로 25ms를 사용한다. \n",
    "            n_fft = Frame length x Sample Rate\n",
    "            ex) 1. Sample rate = 4kHz, n_fft = 100 \n",
    "                   Frame length = 100 / 4000 = 0.025\n",
    "                2. Sample rate = 8kHz, Frame length = 0.025\n",
    "                   n_fft = 200\n",
    "            \n",
    "        4. hop_length (Sample Rate 필요)\n",
    "           hop_length는 읽어들이는 보폭을 의미한다. Frame stride의 경우 10ms로 기본적으로 사용한다. \n",
    "           hop_length = Sample Rate x Frame stride \n",
    "           ex) 8kHz x 10ms = 8000 x 0.01 = 80 (hop_length)\n",
    "'''\n",
    "        # num_channels : 2, sample_rate : 44100, bit_depth : 16\n",
    "def pre_extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        \n",
    "        #FFT Fourier Transform for Calculate Power Spectrum\n",
    "        fft = np.fft.fft(audio)\n",
    "        signal_spectrum = np.abs(fft)\n",
    "        \n",
    "        # Frequency Feature Extract\n",
    "        #f = np.linspace(0, sample_rate, len(signal_spectrum))\n",
    "        #half_spectrum = signal_spectrum[:int(len(signal_spectrum)/2)]\n",
    "        #half_f = f[:int(len(signal_spectrum)/2)]\n",
    "        \n",
    "        #stft\n",
    "        #hop_length = 512\n",
    "        #n_fft = 2048\n",
    "        n_fft = int(sample_rate * 0.025)\n",
    "        hop_length = int(sample_rate * 0.01)\n",
    "        \n",
    "        frame_stride = float(hop_length)/sample_rate  #  0.01\n",
    "        frame_length = float(n_fft)/sample_rate # 0.025\n",
    "        \n",
    "        stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "        spectrogram = np.abs(stft)\n",
    "        \n",
    "        log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "        \n",
    "        #mfccs = librosa.feature.mfcc(audio, sample_rate, n_fft=n_fft, hop_length=hop_length, n_mfcc=100)\n",
    "        #mfccs = librosa.feature.mfcc(audio, sample_rate, log_spectrogram, 100)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, S=log_spectrogram, n_mfcc=50)\n",
    "        pad_width = 0\n",
    "        \n",
    "        mfccs_pad = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs, mfccs_pad, mfccsscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('/home/park/coding/study/Sound/urban/UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the audio files property and standarized the properties of the audio files then extract features\n",
    "wavfilehelper = WavFileHelper()\n",
    "audio_data = []\n",
    "audio_feature = []\n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath('/home/park/coding/study/Sound/urban/UrbanSound8K/audio'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    # extract the property\n",
    "    proper = wavfilehelper.read_file_properties(file_name)\n",
    "    audio_data.append(proper)\n",
    "    # extract the features\n",
    "    label = row['classID']\n",
    "    #mfccs, mfccs_pad, mfccsscaled = extract_features(file_name)\n",
    "    mfccs, mfccs_pad, mfccsscaled = pre_extract_features(file_name)\n",
    "    audio_feature.append([mfccs, mfccs_pad, mfccsscaled, label])\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "audio_property = pd.DataFrame(audio_data, columns=['num_channels','sample_rate','bit_depth'])\n",
    "audio_feature = pd.DataFrame(audio_feature, columns=['mfccs', 'mfccs_pad','feature','class_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the variables\n",
    "audio_property.to_pickle(\"audio_property.pkl\")\n",
    "audio_feature.to_pickle(\"audio_feature.pkl\")\n",
    "\n",
    "# read the variables\n",
    "audio_property = pd.read_pickle(\"audio_property.pkl\")\n",
    "audio_feature = pd.read_pickle(\"audio_feature.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the MFCC Spectrogram for each of the class of the sound data, comparing the graph\n",
    "# https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# choose a sample of sound from each class\n",
    "sound_class_dic = metadata.groupby(['class', 'classID']).size()\n",
    "sound_sample = []\n",
    "for c in range(10):\n",
    "    for index, row in audio_feature.iterrows():\n",
    "        if c == row['class_label']:\n",
    "            sound_sample.append(row)\n",
    "            break\n",
    "\n",
    "# Draw the MFCC Spectrogram for each class\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.subplots_adjust(hspace = 0.5, wspace = 0.5)\n",
    "plt.title('MFCC for different class of sounds on log Hz', y = 1.05, fontsize = 18)\n",
    "plt.axis('off')\n",
    "for i in range(10):\n",
    "    fig.add_subplot(2,5,i+1)\n",
    "    plt.title(f'{sound_class_dic.index[i][0]}')\n",
    "    librosa.display.specshow(sound_sample[i]['mfccs'], x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "for i in range(10):\n",
    "    print(f\"label {i} : {sound_class_dic.index[i][0]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical text data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "# ??? what suppose to be the input of CNN\n",
    "X = np.array(audio_feature.feature.tolist())\n",
    "y = np.array(audio_feature.class_label.tolist())\n",
    "\n",
    "#X_2D = np.array(audio_feature.mfccs_pad.tolist())\n",
    "#_2D = np.array(audio_feature.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "#yy_2D = to_categorical(le.fit_transform(y_2D)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input data\n",
    "# split the data into train and test data sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "#x_train_2D, x_test_2D, y_train_2D, y_test_2D = train_test_split(X_2D, yy_2D, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the hyperparameter space to find the best model\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5],'C': [1, 10 ,20,30,40,50]}]\n",
    "#                    , {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "searchpara = GridSearchCV(svm.SVC(), tuned_parameters)\n",
    "searchpara.fit(x_train, y_train)\n",
    "searchpara.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# construct the model by the best hyperparameters found above\n",
    "SVM = svm.SVC(C=40.0, gamma=0.0001)\n",
    "\n",
    "# train the model\n",
    "SVM.fit(x_train,y_train)\n",
    "\n",
    "save_model = pickle.dumps(SVM)\n",
    "joblib.dump(SVM,'/home/park/coding/study/Sound/urban/Sound_classification_urbansound8k-master/model/svm')\n",
    "\n",
    "y_pred = SVM.predict(x_test)\n",
    "\n",
    "train_accu =  SVM.score(x_train, y_train)\n",
    "test_accu = SVM.score(x_test, y_test)\n",
    "print('Training Accuracy:', train_accu)\n",
    "print('Test Accuracy:', test_accu)\n",
    "\n",
    "print(\"y_pred : \",y_pred)\n",
    "print(\"y_pred shape: \",y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav file load \n",
    "input_audio_feature = []\n",
    "input_sound_sample = []\n",
    "\n",
    "def file_select(num_of_fold):\n",
    "    FOLD_PATH = \"/home/park/coding/study/Sound/urban/UrbanSound8K/audio/test\"\n",
    "    FILE_NAME = FOLD_PATH + str(num_of_fold) + \"/air.wav\"\n",
    "    return FILE_NAME\n",
    "\n",
    "input_file_name = file_select(1)\n",
    "\n",
    "#pre_extract_feature\n",
    "input_signal, input_sample_rate = librosa.load(input_file_name) #7061-6-0-0.wav file\n",
    "input_mfccs, input_mfccs_pad, input_mfccsscaled = pre_extract_features(input_file_name)\n",
    "input_audio_feature.append([input_mfccs, input_mfccs_pad, input_mfccsscaled])\n",
    "\n",
    "\n",
    "input_audio_feature = pd.DataFrame(input_audio_feature, columns=['mfccs', 'mfccs_pad','feature'])\n",
    "\n",
    "input_audio_feature.to_pickle(\"input_audio_feature.pkl\")\n",
    "\n",
    "# read the variables\n",
    "input_audio_feature = pd.read_pickle(\"input_audio_feature.pkl\")\n",
    "\n",
    "hop_length = int(input_sample_rate * 0.01)\n",
    "\n",
    "#draw_chart_mfccs(mfccs, input_signal, input_sample_rate, hop_length)\n",
    "\n",
    "x_input = np.array(input_audio_feature.feature.tolist())\n",
    "\n",
    "\n",
    "y_input = SVM.predict(x_input)\n",
    "print(\"y_pred : \",y_input[0])\n",
    "print(\"y_pred : \",y_input)\n",
    "print(\"y_pred shape: \",y_input.shape)\n",
    "\n",
    "#input_accu = SVM.score(x_input, y_input)\n",
    "#print('input Accuracy:', input_accu)\n",
    "\n",
    "'''\n",
    "label 0 : air_conditioner\n",
    "label 1 : car_horn\n",
    "label 2 : children_playing\n",
    "label 3 : dog_bark\n",
    "label 4 : drilling\n",
    "label 5 : engine_idling\n",
    "label 6 : gun_shot\n",
    "label 7 : jackhammer\n",
    "label 8 : siren\n",
    "label 9 : street_music\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw confusion matrix\n",
    "# new ver: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "# old ver: https://scikit-learn.org/0.21/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          percentage = False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    print(\"cm[0] : \", cm[0])\n",
    "    print(\"y_true :\", y_true)\n",
    "    print(\"y_pred :\", y_pred)\n",
    "    print(\"classes :\", classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8,8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "class_names = metadata.groupby(['class', 'classID'], as_index = False).sum()['class']\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, cmap=plt.cm.Blues, normalize= False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
